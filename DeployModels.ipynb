{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeployModels.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+aKuILXzNNrdNB5fZSTJG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanishkad123/Covid19Visualization/blob/master/DeployModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3CxdJjm5eXN"
      },
      "source": [
        "!pip install colabcode\r\n",
        "!pip install fastapi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmNCi7Yf8Ll8",
        "outputId": "25200104-bee5-4a37-d346-8076e2ee4bab"
      },
      "source": [
        "import re\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import pandas as pd"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwQJDbDbxq6x"
      },
      "source": [
        "from pydantic import BaseModel\r\n",
        "class InterruptionReason(BaseModel):\r\n",
        "    interruption: str \r\n",
        "    correction: str\r\n",
        "    class Config:\r\n",
        "        schema_extra = {\r\n",
        "            \"example\": {\r\n",
        "                \"interruption\": 'RETURED TO THE FIELD RT OVERPRESSURZIED AND THE LT PACK DEFERRED RT PACK HIGH PRESS CAUTION MESSAGE',\r\n",
        "                \"correction\":\"SEA MAINT PERFORMED AN OPERATIONAL CHECK OF THE PACK IN VARIOUS SETTINGS, PACK OPS CHECKS GOOD\"\r\n",
        "            }\r\n",
        "        }\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlB4UY3V7hnb"
      },
      "source": [
        "stemmer = PorterStemmer()\r\n",
        "def identify_tokens1(interruption):\r\n",
        "    comment = re.sub(r'\\d',' ',interruption)\r\n",
        "    words = nltk.word_tokenize(interruption)\r\n",
        "    new_words = [stemmer.stem(word) for word in words]\r\n",
        "    comment = ' '.join(new_words)\r\n",
        "    return comment"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP-90aZj5S47"
      },
      "source": [
        "from fastapi import FastAPI\r\n",
        "import pickle\r\n",
        "\r\n",
        "app = FastAPI()\r\n",
        "\r\n",
        "@app.on_event(\"startup\")\r\n",
        "def load_model():\r\n",
        "    global model\r\n",
        "    model = pickle.load(open(\"linearsvc_model.pkl\", \"rb\"))\r\n",
        "\r\n",
        "@app.get('/')\r\n",
        "def index():\r\n",
        "    return {'message': 'This is the homepage of the API '}\r\n",
        "\r\n",
        "\r\n",
        "@app.post('/predict')\r\n",
        "def get_music_category(data: InterruptionReason):\r\n",
        "    received = data.dict()\r\n",
        "    interruption = received['interruption']\r\n",
        "    correction = received['correction']\r\n",
        "    #prediction_list = model.predict([identify_tokens1(interruption + ' ' + correction)]).tolist()\r\n",
        "    prob = pd.DataFrame(model.predict_proba([identify_tokens1(interruption + ' ' + correction)]), columns=model.classes_)\r\n",
        "    pred = prob.sort_values(by=0,axis=1,ascending=False).iloc[:, 0:3]\r\n",
        "    predicted = '1. '+ str(pred.columns[0]) + ' : ' + str(pred._get_value(0, pred.columns[0])) + ' 2. ' + str(pred.columns[1]) + ' : ' + str(pred._get_value(0, pred.columns[1])) + ' 3. ' + str(pred.columns[2]) + ' : ' + str(pred._get_value(0, pred.columns[2]))\r\n",
        "    \r\n",
        "    return {'prediction': predicted}"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r1pifv-8toJ"
      },
      "source": [
        "from colabcode import ColabCode\r\n",
        "server = ColabCode(port=10000, code=False)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZPBzTQw8xKV",
        "outputId": "f25799f9-04a9-4f86-bdf6-6c530da1d80e"
      },
      "source": [
        "server.run_app(app=app)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: NgrokTunnel: \"http://1e35df29d6ec.ngrok.io\" -> \"http://localhost:10000\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [62]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:10000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     2607:fea8:239f:b780:f947:351d:765f:4f59:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     99.252.137.35:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     99.252.137.35:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     99.252.137.35:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     99.252.137.35:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     99.252.137.35:0 - \"POST /predict HTTP/1.1\" 200 OK\n",
            "INFO:     2605:8d80:6c0:5abb:9a58:c6b:624f:b129:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2605:8d80:6c0:5abb:9a58:c6b:624f:b129:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     99.252.137.35:0 - \"POST /predict HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [62]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}